{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mktkcO3vNz-"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9KyPFFFc9k4"
      },
      "outputs": [],
      "source": [
        "!pip install -q torchinfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgtCabChpBSv"
      },
      "outputs": [],
      "source": [
        "#%cd \"/content/drive/MyDrive/Colab Notebooks/Projects/BUSI_bin_augmented\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rDoavFXyIBdz"
      },
      "outputs": [],
      "source": [
        "#@title Imports\n",
        "import os\n",
        "import cv2\n",
        "import random\n",
        "import albumentations as A\n",
        "from collections import Counter\n",
        "from PIL import Image, ImageChops, ImageDraw, ImageFilter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "from sklearn.model_selection import KFold\n",
        "from torchinfo import summary\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm_notebook\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jS20dTmWWLss"
      },
      "outputs": [],
      "source": [
        "# Setting the seed\n",
        "random.seed(42)\n",
        "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRevbwX-s5af"
      },
      "outputs": [],
      "source": [
        "data_dir = \"/content/drive/MyDrive/Datasets/BUSI_bin_augmented\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23oTqdUex9Dy"
      },
      "outputs": [],
      "source": [
        "subfolders = ['Benign','Malignant']\n",
        "ben_dir = os.path.join(data_dir, subfolders[0])\n",
        "mal_dir = os.path.join(data_dir, subfolders[1])\n",
        "\n",
        "_, _, ben_files = next(os.walk(ben_dir))\n",
        "_, _, mal_files = next(os.walk(mal_dir))\n",
        "\n",
        "print(f\"Number of benign images: {len(ben_files)}\")\n",
        "print(f\"Number of malignant images: {len(mal_files)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EfPaYgQ1Mj_7"
      },
      "outputs": [],
      "source": [
        "#@title Data Loading and Preprocessing\n",
        "# Set the preprocess operations to be performed on train/val/test samples\n",
        "# Define the transform operations\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize to 224x224\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "dataset = datasets.ImageFolder(root=data_dir, transform=preprocess)\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = int(0.1 * len(dataset))\n",
        "test_size = len(dataset) - train_size - val_size\n",
        "\n",
        "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [train_size, val_size, test_size], generator=g)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, generator=g)\n",
        "val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=False, generator=g)\n",
        "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False, generator=g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63-1ZhPabkHv"
      },
      "outputs": [],
      "source": [
        "# Visualize some examples\n",
        "NUM_IMAGES = 4\n",
        "examples = torch.stack([val_set[idx][0] for idx in range(NUM_IMAGES)], dim=0)\n",
        "img_grid = torchvision.utils.make_grid(examples, nrow=2, normalize=True, pad_value=0.9)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.title(\"Sample images in dataset\")\n",
        "plt.imshow(img_grid)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w06tAiSbaBKl"
      },
      "outputs": [],
      "source": [
        "# Print the dimension of images to verify all loaders have the same dimensions\n",
        "def print_dim(loader, text):\n",
        "    print('---------'+text+'---------')\n",
        "    print(len(loader.dataset))\n",
        "    for image, label in loader:\n",
        "        print(image.shape)\n",
        "        print(label.shape)\n",
        "        break\n",
        "\n",
        "print_dim(train_loader,'training loader')\n",
        "print_dim(val_loader,'validation loader')\n",
        "print_dim(test_loader,'test loader')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2R9apZ6rbNC"
      },
      "outputs": [],
      "source": [
        "def img_to_patch(x, patch_size, flatten_channels=True):\n",
        "    \"\"\"\n",
        "    Convert images to patches.\n",
        "\n",
        "    Args:\n",
        "        x (torch.Tensor): Tensor representing the image of shape [B, C, H, W]\n",
        "        patch_size (int): Number of pixels per dimension of the patches\n",
        "        flatten_channels (bool): If True, flatten the patches into feature vectors\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Patched images\n",
        "    \"\"\"\n",
        "    B, C, H, W = x.shape\n",
        "    assert H % patch_size == 0 and W % patch_size == 0, \"Image dimensions must be divisible by patch size\"\n",
        "\n",
        "    x = x.reshape(B, C, H // patch_size, patch_size, W // patch_size, patch_size)\n",
        "    x = x.permute(0, 2, 4, 1, 3, 5)\n",
        "    x = x.flatten(1, 2)\n",
        "\n",
        "    if flatten_channels:\n",
        "        x = x.flatten(2, 4)\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGVi3zUfHcLW"
      },
      "outputs": [],
      "source": [
        "def visualize_patches(images, patch_size, image_size, titles=None):\n",
        "    img_patches = img_to_patch(images, patch_size=patch_size, flatten_channels=False)\n",
        "\n",
        "    fig, axes = plt.subplots(2, images.shape[0], figsize=(20, 10))\n",
        "\n",
        "    if images.shape[0] == 1:\n",
        "        axes = axes.reshape(2, -1)\n",
        "\n",
        "    # fig.suptitle(\"Original Images and Their Patches\", fontsize=16)\n",
        "\n",
        "    for i in range(images.shape[0]):\n",
        "        # Plot original image\n",
        "        orig_img = images[i].permute(1, 2, 0)  # Change shape from (C, H, W) to (H, W, C)\n",
        "        axes[0, i].imshow(orig_img)\n",
        "        axes[0, i].axis(\"off\")\n",
        "        axes[0, i].set_title(f\"Original Image: {titles[i] if titles else ''}\", fontsize=12)\n",
        "\n",
        "        # Plot image patches\n",
        "        img_grid = torchvision.utils.make_grid(img_patches[i], nrow=image_size//patch_size, normalize=True, pad_value=0.9)\n",
        "        img_grid = img_grid.permute(1, 2, 0)  # Change shape from (C, H, W) to (H, W, C)\n",
        "        axes[1, i].imshow(img_grid)\n",
        "        axes[1, i].axis(\"off\")\n",
        "        axes[1, i].set_title(f\"Image Patches: {titles[i] if titles else ''}\", fontsize=12)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iB2OQuANTPMi"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_image(image_path, target_size):\n",
        "    \"\"\"\n",
        "    Load and preprocess an image file.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): Path to the image file\n",
        "        target_size (tuple): Target size for resizing (height, width)\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Preprocessed image tensor\n",
        "    \"\"\"\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img = img.resize(target_size)\n",
        "    img_tensor = torchvision.transforms.ToTensor()(img)\n",
        "    return img_tensor.unsqueeze(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--qAhSkyZp2a"
      },
      "outputs": [],
      "source": [
        "def process_images(img_names, img_dir, target_size):\n",
        "    img_tensors = []\n",
        "    for img_name in img_names:\n",
        "        img_path = os.path.join(img_dir, img_name)\n",
        "        img_tensor = load_and_preprocess_image(img_path, target_size)\n",
        "        img_tensors.append(img_tensor)\n",
        "    return torch.stack(img_tensors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7jzQccbQYZl"
      },
      "outputs": [],
      "source": [
        "image_size = 224\n",
        "patch_size = 16\n",
        "num_patches = (image_size // patch_size) ** 2\n",
        "num_channels = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkLqzOr50cKo"
      },
      "outputs": [],
      "source": [
        "ben_imgs = [f for f in os.listdir(ben_dir) if f.endswith('.bmp')][:2]\n",
        "mal_imgs = [f for f in os.listdir(mal_dir) if f.endswith('.bmp')][:2]\n",
        "\n",
        "image_titles = ben_imgs + mal_imgs\n",
        "\n",
        "# Load and preprocess images\n",
        "images = []\n",
        "for img_name in ben_imgs:\n",
        "    img_path = os.path.join(ben_dir, img_name)\n",
        "    img_tensor = load_and_preprocess_image(img_path, (image_size, image_size))\n",
        "    images.append(img_tensor)\n",
        "\n",
        "for img_name in mal_imgs:\n",
        "    img_path = os.path.join(mal_dir, img_name)\n",
        "    img_tensor = load_and_preprocess_image(img_path, (image_size, image_size))\n",
        "    images.append(img_tensor)\n",
        "\n",
        "# Combine images into a single tensor\n",
        "samples = torch.cat(images, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7ziwXXST0i7"
      },
      "outputs": [],
      "source": [
        "# Visualize patches\n",
        "visualize_patches(samples, patch_size, image_size, titles=image_titles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZuCQKxGMbGW"
      },
      "source": [
        "# Modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UxZza3AhVpmK"
      },
      "outputs": [],
      "source": [
        "#@title Helper Functions\n",
        "# Early stopping class\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=5, delta=0):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.counter = 0\n",
        "        self.best_loss = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_loss):\n",
        "        if self.best_loss is None or val_loss < self.best_loss - self.delta:\n",
        "            self.best_loss = val_loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UVkpEQMeORNd"
      },
      "outputs": [],
      "source": [
        "# Pretrained ViT setup\n",
        "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.IMAGENET1K_SWAG_LINEAR_V1 #ViT_B_16_Weights, ViT_H_14_Weights, ViT_H_14_Weights\n",
        "pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device) # vit_b_16, vit_h_14, vit_l_16\n",
        "\n",
        "# Freeze base parameters\n",
        "for parameter in pretrained_vit.parameters():\n",
        "    parameter.requires_grad = False\n",
        "\n",
        "# Replace the classification head\n",
        "num_classes = len(subfolders)  # Two classes: Malignant and Benign\n",
        "pretrained_vit.heads = nn.Linear(in_features=768, out_features=2).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QeTg2GJZe2No"
      },
      "outputs": [],
      "source": [
        "# Get automatic transforms from pretrained ViT weights\n",
        "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
        "print(pretrained_vit_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqPePGXJN222"
      },
      "outputs": [],
      "source": [
        "# Print a summary using torchinfo (uncomment for actual output)\n",
        "summary(model=pretrained_vit,\n",
        "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
        "        # col_names=[\"input_size\"], # smaller output\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7pxXQ6Ce9SH"
      },
      "outputs": [],
      "source": [
        "# Function to create dataloaders\n",
        "def create_dataloaders(data_dir: str, batch_size: int, transform: transforms.Compose, num_workers: int = os.cpu_count()):\n",
        "    # Load dataset\n",
        "    dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
        "\n",
        "    # Split the dataset into train, validation, and test sets\n",
        "    train_size = int(0.7 * len(dataset))\n",
        "    print(f\"Train size: {train_size}\")\n",
        "    val_size = int(0.2 * len(dataset))\n",
        "    print(f\"Val size: {val_size}\")\n",
        "    test_size = len(dataset) - train_size - val_size\n",
        "    print(f\"Test size: {test_size}\")\n",
        "\n",
        "    class_names = dataset.classes\n",
        "    class_counts = [0] * len(class_names)\n",
        "    for _, label in dataset:\n",
        "        class_counts[label] += 1\n",
        "\n",
        "    print(\"Class counts:\")\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        print(f\"{class_name}: {class_counts[i]}\")\n",
        "\n",
        "    train_set, val_set, test_set = torch.utils.data.random_split(dataset, [train_size, val_size, test_size], generator=torch.Generator().manual_seed(23))\n",
        "\n",
        "    # Turn datasets into DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        train_set,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_set,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_set,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iISoVl63E_nr"
      },
      "outputs": [],
      "source": [
        "os.cpu_count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBHHmeYKa_3V"
      },
      "source": [
        "# Pretrained Vision Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQ53wDySBQSa"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, test_dataloader, loss_fn, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    test_loss = 0.0\n",
        "    correct_predictions = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            test_loss += loss.item() * images.size(0)  # Accumulate loss\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct_predictions += torch.sum(preds == labels.data).item()\n",
        "            total_samples += labels.size(0)\n",
        "\n",
        "    average_loss = test_loss / total_samples\n",
        "    accuracy = correct_predictions / total_samples\n",
        "\n",
        "    # print(f\"Test Loss: {average_loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    return average_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wXe9OUaMtq0"
      },
      "outputs": [],
      "source": [
        "def train(model, train_dataloader, val_dataloader, test_dataloader, loss_fn, optimizer, device, epochs, early_stopping):\n",
        "    metrics = {\n",
        "        'train_losses': [],\n",
        "        'val_losses': [],\n",
        "        'train_accuracies': [],\n",
        "        'val_accuracies': [],\n",
        "        'test_losses': [],\n",
        "        'test_accuracies': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
        "\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        training_loss = 0.0\n",
        "        train_corrects = 0\n",
        "        total_train_samples = 0\n",
        "        for images, labels in tqdm(train_dataloader):\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            training_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            train_corrects += torch.sum(preds == labels.data).item()\n",
        "            total_train_samples += labels.size(0)\n",
        "\n",
        "        training_loss /= total_train_samples\n",
        "        train_acc = train_corrects / total_train_samples\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_corrects = 0\n",
        "        total_val_samples = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_dataloader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = loss_fn(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                _, preds = torch.max(outputs, 1)\n",
        "                val_corrects += torch.sum(preds == labels.data).item()\n",
        "                total_val_samples += labels.size(0)\n",
        "\n",
        "        val_loss /= total_val_samples\n",
        "        val_acc = val_corrects / total_val_samples\n",
        "\n",
        "        test_loss, test_accuracy = evaluate(model, test_dataloader, loss_fn, device)\n",
        "\n",
        "        metrics['train_losses'].append(training_loss)\n",
        "        metrics['val_losses'].append(val_loss)\n",
        "        metrics['train_accuracies'].append(train_acc)\n",
        "        metrics['val_accuracies'].append(val_acc)\n",
        "        metrics['test_losses'].append(test_loss)\n",
        "        metrics['test_accuracies'].append(test_accuracy)\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], \"\n",
        "              f\"Train Loss: {training_loss:.4f}, Train Accuracy: {train_acc:.4f}, \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}, \"\n",
        "              f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "        if early_stopping:\n",
        "            early_stopping(val_loss)\n",
        "            if early_stopping.early_stop:\n",
        "                print(\"Early stopping\")\n",
        "                break\n",
        "\n",
        "    return model, metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pql-qzzBOd-J"
      },
      "outputs": [],
      "source": [
        "#@title Hyperparameters\n",
        "learning_rate = 1e-4\n",
        "optimizer = torch.optim.Adam(params=pretrained_vit.parameters(), lr=learning_rate)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "num_epochs = 80\n",
        "early_stopping_patience = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDIynUA7JJH5"
      },
      "outputs": [],
      "source": [
        "#@title Training Loop\n",
        "early_stopping = EarlyStopping(patience=early_stopping_patience)\n",
        "\n",
        "# Train the classifier head of the pretrained ViT\n",
        "pretrained_vit, pretrained_vit_metrics = train(\n",
        "    model=pretrained_vit,\n",
        "    train_dataloader=train_loader,\n",
        "    val_dataloader=val_loader,\n",
        "    test_dataloader=test_loader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    epochs=num_epochs,\n",
        "    device=device,\n",
        "    early_stopping=early_stopping\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZAiONUqEYF5"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(metrics, title):\n",
        "    epochs = range(1, len(metrics['train_losses']) + 1)\n",
        "\n",
        "    fig = make_subplots(rows=1, cols=2, subplot_titles=(f'Loss', f'Accuracy'))\n",
        "\n",
        "    # Plot loss\n",
        "    fig.add_trace(go.Scatter(x=list(epochs), y=metrics['train_losses'], mode='lines', name='Training Loss'), row=1, col=1)\n",
        "    fig.add_trace(go.Scatter(x=list(epochs), y=metrics['val_losses'], mode='lines', name='Validation Loss'), row=1, col=1)\n",
        "    fig.add_trace(go.Scatter(x=[epochs[0], epochs[-1]], y=[metrics['test_losses'][-1], metrics['test_losses'][-1]], mode='lines', name='Test Loss', line=dict(dash='dash', color='red')), row=1, col=1)\n",
        "\n",
        "    # Plot accuracy\n",
        "    fig.add_trace(go.Scatter(x=list(epochs), y=metrics['train_accuracies'], mode='lines', name='Training Accuracy'), row=1, col=2)\n",
        "    fig.add_trace(go.Scatter(x=list(epochs), y=metrics['val_accuracies'], mode='lines', name='Validation Accuracy'), row=1, col=2)\n",
        "    fig.add_trace(go.Scatter(x=[epochs[0], epochs[-1]], y=[metrics['test_accuracies'][-1], metrics['test_accuracies'][-1]], mode='lines', name='Test Accuracy', line=dict(dash='dash', color='red')), row=1, col=2)\n",
        "\n",
        "    fig.update_xaxes(title_text=\"Epoch\", row=1, col=1)\n",
        "    fig.update_yaxes(title_text=\"Loss\", row=1, col=1)\n",
        "    fig.update_xaxes(title_text=\"Epoch\", row=1, col=2)\n",
        "    fig.update_yaxes(title_text=\"Accuracy\", row=1, col=2)\n",
        "\n",
        "    fig.update_layout(title_text=title, showlegend=True, width=1000, height=500)\n",
        "\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RYNbXiiyEaUq"
      },
      "outputs": [],
      "source": [
        "# Plot training and validation metrics with test metrics\n",
        "plot_metrics(pretrained_vit_metrics, \"Training, Validation, and Test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nu7BbqDHCphj"
      },
      "outputs": [],
      "source": [
        "def plot_roc_curve(model, dataloader, device, title=\"ROC Curve\"):\n",
        "    model.eval()\n",
        "    y_true = []\n",
        "    y_scores = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_scores.extend(probabilities.cpu().numpy()[:, 1])  # binary classification and we're interested in class 1\n",
        "\n",
        "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title(title)\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FOT6wGnDF2g"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve for the test set\n",
        "plot_roc_curve(pretrained_vit, test_loader, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brpecADnDNIf"
      },
      "outputs": [],
      "source": [
        "#@title Saving the model\n",
        "def save_model(model, path):\n",
        "    torch.save(model.state_dict(), path)\n",
        "    print(f\"Model saved to {path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4DvBu50psp4"
      },
      "outputs": [],
      "source": [
        "model_path = \"/content/drive/MyDrive/Results and output/Breast Cancer Transformer (BCT)\"\n",
        "save_model(pretrained_vit, model_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
